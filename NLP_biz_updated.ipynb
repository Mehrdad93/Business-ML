{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WS_biz_updated.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehrdad93/Business-ML/blob/master/NLP_biz_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rToCGf_R9iNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEf3pvWN6H5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Developed by Mehrdad Mokhtari\n",
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1e10\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.offline as plt2\n",
        "import plotly.graph_objs as go\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.neighbors import KDTree\n",
        "from itertools import cycle\n",
        "from wordcloud import WordCloud, ImageColorGenerator\n",
        "\n",
        "!pip install adjustText\n",
        "from adjustText import adjust_text\n",
        "import multiprocessing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KcgxGk96NDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/WS_nlp/data_full.csv', engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
        "l1 = len(df)\n",
        "# removing the first year of dataframe\n",
        "df.sort_values(by='Date')\n",
        "df = df[pd.to_datetime(df['Date'], errors='coerce') >= pd.to_datetime('2013-01-01')]\n",
        "l2 = len(df)\n",
        "print(\"The percentage of dataframe that was removed in %\", (l1-l2)*100/l1)\n",
        "# df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzVLnm0f-mwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove threads which contain \"Ask me anything\" or \"AMA\" with diff. variations\n",
        "df = df[~df[\"Title\"].str.contains(\"Ask me anything\", na=False)]\n",
        "df = df[~df[\"Title\"].str.contains(\"ask me anything\", na=False)]\n",
        "df = df[~df[\"Title\"].str.contains(\"Ask Me Anything\", na=False)]\n",
        "df = df[~df[\"Title\"].str.contains(\" AMA \", na=False)]\n",
        "df = df[~df[\"Title\"].str.contains(\"[AMA]\", na=False)]\n",
        "df = df[~df[\"Title\"].str.contains(\"(AMA)\", na=False)]\n",
        "\n",
        "l3 = len(df)\n",
        "print(\"The percentage of dataframe that was removed in %\", (l2-l3)*100/l2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkDk9GXO6NJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove all titles with NAN\n",
        "title_data = df[df['Title'].notnull()]\n",
        "# Grab all the titles and conversations \n",
        "reddit_titles = title_data['Title']\n",
        "reddit_conv = title_data['Conversation']\n",
        "# Create a list of strings, one for each title and conversation\n",
        "titles_list = [title for title in reddit_titles]\n",
        "conv_list = [title for title in reddit_conv]\n",
        "# Collapse the list of strings into a single long string for processing\n",
        "big_title_string = ' '.join(titles_list)\n",
        "big_conv_string = ' '.join(map(str, conv_list))\n",
        "\n",
        "# Tokenize the string into words: creating bag of words\n",
        "tokens_titles = word_tokenize(big_title_string)\n",
        "tokens_conv = word_tokenize(big_conv_string)\n",
        "\n",
        "# Remove non-alphabetic tokens, such as punctuation\n",
        "words_titles = [word.lower() for word in tokens_titles if word.isalpha()]\n",
        "words_conv = [word.lower() for word in tokens_conv if word.isalpha()]\n",
        "\n",
        "# Filter out stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "# Some words are added to stopwords\n",
        "newStopWords = ['one','though','anyway','anyways','like','even','thought','thing',\n",
        "                'etc','cuz','dude','hey','hahah','haha','lol', 'mid', 'aa'] \n",
        "stop_words.extend(newStopWords)\n",
        "\n",
        "words_titles = [word for word in words_titles if not word in stop_words]\n",
        "words_conv = [word for word in words_conv if not word in stop_words]\n",
        "\n",
        "# remove duplicate words:\n",
        "seen = set()\n",
        "words_conv_new = []\n",
        "for item in words_conv:\n",
        "    if item not in seen:\n",
        "        seen.add(item)\n",
        "        words_conv_new.append(item)\n",
        "\n",
        "seen2 = set()\n",
        "words_titles_new = []\n",
        "for item2 in words_titles:\n",
        "    if item2 not in seen2:\n",
        "        seen2.add(item2)\n",
        "        words_titles_new.append(item2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trOfsYGwSC4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replier location and occupation\n",
        "# 1\n",
        "# create a dataset with non NAN values for replies\n",
        "replier_data = df[df['Replier'].notnull()]\n",
        "# adding two new columns \n",
        "replier_data['Location'], replier_data['Occupation'] = [np.nan, np.nan]\n",
        "replier_conv = replier_data['Conversation']\n",
        "replier_conv_list = [title for title in replier_conv]\n",
        "print(replier_conv_list[1])\n",
        "\n",
        "replier_tokens_conv = [0]*len(replier_conv_list)\n",
        "for i in range(len(replier_conv_list)):\n",
        "    # list in a list\n",
        "    replier_tokens_conv[i] = word_tokenize(str(replier_conv_list[i]))\n",
        "\n",
        "replier_words_conv = [0]*len(replier_conv_list)\n",
        "for i in range(len(replier_conv_list)):\n",
        "    replier_words_conv[i] = [word.lower() for word in replier_tokens_conv[i] if word.isalpha()]\n",
        "    # print('-'*50)\n",
        "    replier_words_conv[i] = [word for word in replier_words_conv[i] if not word in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG14STD0k9--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2\n",
        "# Occupation: simil_final_5\n",
        "temp = [0]*len(replier_conv_list)\n",
        "for i in range(len(replier_conv_list)):\n",
        "    temp[i] = [word for word in simil_final_5 if word in replier_words_conv[i]]\n",
        "for i in range(len(replier_conv_list)):\n",
        "    if not temp[i]:\n",
        "       replier_data['Occupation'].values[i] = 0\n",
        "    else:\n",
        "       replier_data['Occupation'].values[i] = 1\n",
        "\n",
        "# US cities/states: simil_final_6 \n",
        "temp = [0]*len(replier_conv_list)\n",
        "for i in range(len(replier_conv_list)):\n",
        "    temp[i] = [word for word in simil_final_6 if word in replier_words_conv[i]]\n",
        "for i in range(len(replier_conv_list)):\n",
        "    if not temp[i]:\n",
        "       replier_data['Location'].values[i] = 0\n",
        "    else:\n",
        "       replier_data['Location'].values[i] = 1 \n",
        "\n",
        "# adding two new variables to the main dataset just for replies\n",
        "out = replier_data.to_csv('output.csv', index = None, header=True)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFZyIhWuC-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 just conv of seekers\n",
        "title_data = df[df['Title'].notnull()]\n",
        "# Grab all the titles and conversations \n",
        "reddit_conv = title_data['Conversation']\n",
        "# Create a list of strings, one for each title and conversation\n",
        "conv_list = [title for title in reddit_conv]\n",
        "\n",
        "title_data['Location'], title_data['Occupation'] = [np.nan, np.nan]\n",
        "seeker_tokens_conv = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    # list in a list\n",
        "    seeker_tokens_conv[i] = word_tokenize(str(conv_list[i]))\n",
        "\n",
        "seeker_words_conv = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    seeker_words_conv[i] = [word.lower() for word in seeker_tokens_conv[i] if word.isalpha()]\n",
        "    # print('-'*50)\n",
        "    seeker_words_conv[i] = [word for word in seeker_words_conv[i] if not word in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ion0Mi0p5HPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2\n",
        "# Occupation: simil_final_5\n",
        "temp = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    temp[i] = [word for word in simil_final_5 if word in seeker_words_conv[i]]\n",
        "for i in range(len(conv_list)):\n",
        "    if not temp[i]:\n",
        "       title_data['Occupation'].values[i] = 0\n",
        "    else:\n",
        "       title_data['Occupation'].values[i] = 1\n",
        "\n",
        "# US cities/states: simil_final_6 \n",
        "temp = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    temp[i] = [word for word in simil_final_6 if word in seeker_words_conv[i]]\n",
        "for i in range(len(conv_list)):\n",
        "    if not temp[i]:\n",
        "       title_data['Location'].values[i] = 0\n",
        "    else:\n",
        "       title_data['Location'].values[i] = 1 \n",
        "\n",
        "# adding two new variables to the main dataset just for seekers questions\n",
        "out2 = title_data.to_csv('output2.csv', index = None, header=True)  \n",
        "title_data[\"Location\"].mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzGy8jut6CaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 conv of entire seekres and repliers-total\n",
        "reddit_conv = df['Conversation']\n",
        "# Create a list of strings, one for each title and conversation\n",
        "conv_list = [title for title in reddit_conv]\n",
        "\n",
        "df['Location'], df['Occupation'] = [np.nan, np.nan]\n",
        "seeker_tokens_conv = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    # list in a list\n",
        "    seeker_tokens_conv[i] = word_tokenize(str(conv_list[i]))\n",
        "\n",
        "seeker_words_conv = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    seeker_words_conv[i] = [word.lower() for word in seeker_tokens_conv[i] if word.isalpha()]\n",
        "    # print('-'*50)\n",
        "    seeker_words_conv[i] = [word for word in seeker_words_conv[i] if not word in stop_words]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am6c5Hmf0Osf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2-total\n",
        "# Occupation: simil_final_5\n",
        "temp = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    temp[i] = [word for word in simil_final_5 if word in seeker_words_conv[i]]\n",
        "for i in range(len(conv_list)):\n",
        "    if not temp[i]:\n",
        "       df['Occupation'].values[i] = 0\n",
        "    else:\n",
        "       df['Occupation'].values[i] = 1\n",
        "\n",
        "# US cities/states: simil_final_6 \n",
        "temp = [0]*len(conv_list)\n",
        "for i in range(len(conv_list)):\n",
        "    temp[i] = [word for word in simil_final_6 if word in seeker_words_conv[i]]\n",
        "for i in range(len(conv_list)):\n",
        "    if not temp[i]:\n",
        "       df['Location'].values[i] = 0\n",
        "    else:\n",
        "       df['Location'].values[i] = 1 \n",
        "\n",
        "# Entire dataset\n",
        "out3 = df.to_csv('output3.csv', index = None, header=True)  \n",
        "df[\"Location\"].mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cee9S1KLfYed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NEW\n",
        "# create a new list of words which excludes not original locations\n",
        "# can be expanded!\n",
        "excluded_loc_1 = [\"order\", \"source\", \"export\", \"purchase\"]\n",
        "excluded_loc_2 = [\"to\", \"from\"]\n",
        "\n",
        "for i in range(len(conv_list)):\n",
        "  for word in seeker_words_conv[i]:\n",
        "    if word in excluded_loc_1:\n",
        "      if word in excluded_loc_2:\n",
        "        df['Location'].values[i] = 0\n",
        "        #print(\"changed\")\n",
        "        continue\n",
        "\n",
        "out4 = df.to_csv('output4.csv', index = None, header=True) \n",
        "df[\"Location\"].mean()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz8N4H45-kMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_# Load Google's pre-trained Word2Vec model. ~100 billion words!\n",
        "# It might take couple of minutes to be done\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/My Drive/WS_nlp/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz', binary=True) \n",
        "# model = gensim.models.KeyedVectors.load_word2vec_format('http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip', binary=True, encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFvtuOMV-kO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter the list of vectors to include only those that Word2Vec has a vector for\n",
        "vector_list_titles = [model[word] for word in words_titles if word in model.vocab]\n",
        "vector_list_conv = [model[word] for word in words_conv if word in model.vocab]\n",
        "\n",
        "vector_list_conv_new = [model[word] for word in words_conv_new if word in model.vocab]\n",
        "words_filtered_conv_new = [word for word in words_conv_new if word in model.vocab]\n",
        "\n",
        "Vec = np.vstack(vector_list_conv_new) \n",
        "#Vec = np.vstack(vector_list_titles)\n",
        "\n",
        "# Create a list of the words corresponding to these vectors\n",
        "words_filtered_titles = [word for word in words_titles if word in model.vocab]\n",
        "words_filtered_conv = [word for word in words_conv if word in model.vocab]\n",
        "\n",
        "# Zip the words together with their vector representations\n",
        "word_vec_zip_titles = zip(words_filtered_titles, vector_list_titles)\n",
        "word_vec_zip_conv = zip(words_filtered_conv, vector_list_conv)\n",
        "\n",
        "# Cast to a dict so we can turn it into a DataFrame\n",
        "word_vec_dict_titles = dict(word_vec_zip_titles)\n",
        "word_vec_dict_conv = dict(word_vec_zip_conv)\n",
        "df_titles = pd.DataFrame.from_dict(word_vec_dict_titles, orient='index')\n",
        "df_conv = pd.DataFrame.from_dict(word_vec_dict_conv, orient='index')\n",
        "#df_conv.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppTGHAS-cVkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 Similar words: country \n",
        "word_countries = ['canada', 'netherlands', 'denmark','australia','china']\n",
        "simil_1 = model.wv.most_similar(positive=word_countries, negative=['viagra', 'macintosh', 'png', 'api'], topn=100) #opt\n",
        "print('-'*30)\n",
        "first_list = [x[0] for x in simil_1]\n",
        "simil_final_2 = [word for word in first_list if word in words_conv]\n",
        "simil_final_vec = [model[word] for word in first_list if word in words_conv]\n",
        "# pd array \n",
        "Vec_2 = np.vstack(simil_final_vec) \n",
        "print(simil_final_2)\n",
        "print(len(Vec_2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YKOeCqBNDip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 Similar words: season/month\n",
        "word_season = ['january', 'February', 'march', 'summer', 'autumn', 'spring']\n",
        "simil_2 = model.wv.most_similar(positive = word_season, topn = 150) #opt\n",
        "print('-'*30)\n",
        "first_list = [x[0] for x in simil_2]\n",
        "simil_final_3 = [word for word in first_list if word in words_conv]\n",
        "simil_final_vec = [model[word] for word in first_list if word in words_conv]\n",
        "# pd array \n",
        "Vec_3 = np.vstack(simil_final_vec) \n",
        "print(simil_final_3)\n",
        "print(len(Vec_3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JICxIETPRk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 Similar words: disrespectful\n",
        "word_rude = ['crap', 'fuck', 'fucking', 'ass', 'idiot', 'shit', 'asshole']\n",
        "simil_3 = model.wv.most_similar(positive = word_rude, topn = 200) #opt\n",
        "print('-'*30)\n",
        "first_list = [x[0] for x in simil_3]\n",
        "simil_final_4 = [word for word in first_list if word in words_conv]\n",
        "simil_final_vec = [model[word] for word in first_list if word in words_conv]\n",
        "# pd array \n",
        "Vec_4 = np.vstack(simil_final_vec) \n",
        "print(simil_final_4)\n",
        "print(len(Vec_4))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keAePzUY-EME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4 Similar words: occupation\n",
        "word_occupation = ['demolition', 'plumbing','barber','teacher','salesman']\n",
        "simil_4 = model.wv.most_similar(positive = word_occupation, topn = 200) #opt\n",
        "print('-'*30)\n",
        "first_list = [x[0] for x in simil_4]\n",
        "simil_final_5 = [word for word in first_list if word in words_conv]\n",
        "simil_final_vec = [model[word] for word in first_list if word in words_conv]\n",
        "# pd array \n",
        "Vec_5 = np.vstack(simil_final_vec) \n",
        "print(simil_final_5)\n",
        "print(len(Vec_5))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIA9bCVNBH0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5 Similar words: city/state\n",
        "word_city = ['houston','toronto','vancouver','nyc']\n",
        "simil_5 = model.wv.most_similar(positive = word_city, topn = 100) #opt\n",
        "print('-'*30)\n",
        "first_list = [x[0] for x in simil_5]\n",
        "simil_final_6 = [word for word in first_list if word in words_conv]\n",
        "simil_final_vec = [model[word] for word in first_list if word in words_conv]\n",
        "# pd array \n",
        "Vec_6 = np.vstack(simil_final_vec) \n",
        "print(simil_final_6)\n",
        "print(len(Vec_6))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_CeG8vfmcya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dimensionality reduction: Initializing t-SNE to visualize locations and occupations in 2D\n",
        "tsne0 = TSNE(n_components = 2, init = 'pca', random_state = 0, perplexity = 200)\n",
        "\n",
        "tsne_df_conv = tsne0.fit_transform(Vec_5)\n",
        "tsne_df_conv_1 = tsne0.fit_transform(Vec_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMXmGJSpIyco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just tsne_df_conv and df_conv plotted (country+occupation)\n",
        "sns.set()\n",
        "# Initialize figure\n",
        "fig, ax = plt.subplots(figsize = (14, 10))\n",
        "sns.scatterplot(tsne_df_conv[:, 0]+200, tsne_df_conv[:, 1]+200, alpha = 0.5)\n",
        "sns.scatterplot(tsne_df_conv_1[:, 0], tsne_df_conv_1[:, 1], alpha = 0.5)\n",
        "\n",
        "# Initialize list of texts\n",
        "texts = []\n",
        "texts_1 = []\n",
        "words_to_plot = list(np.arange(0, len(Vec_5), 1))\n",
        "words_to_plot_1 = list(np.arange(0, len(Vec_2), 1))\n",
        "\n",
        "# Append words to list\n",
        "for word in words_to_plot:\n",
        "    texts.append(plt.text(tsne_df_conv[word, 0]+200, tsne_df_conv[word, 1]+200, simil_final_5[word], fontsize = 12))\n",
        "\n",
        "for word1 in words_to_plot_1:\n",
        "    texts_1.append(plt.text(tsne_df_conv_1[word1, 0], tsne_df_conv_1[word1, 1], simil_final_2[word1], fontsize = 12))\n",
        "\n",
        "# Plot text using adjust_text (because overlapping text is hard to read)\n",
        "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
        "            expand_points = (2,1), expand_text = (1,2),\n",
        "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
        "\n",
        "adjust_text(texts_1, force_points = 0.4, force_text = 0.4, \n",
        "            expand_points = (2,1), expand_text = (1,2),\n",
        "            arrowprops = dict(arrowstyle = \"-\", color = 'green', lw = 0.5))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY6Vq2UYnYks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3D TSNE for tsne_df_conv and df_conv [new]\n",
        "#Vec = np.vstack(vector_list_conv_new) \n",
        "def get_coordinates(Vec):\n",
        "    tsne = TSNE(n_components=3, perplexity=100, init='random', random_state=0, verbose=1)\n",
        "    # init='random', perplexity=100 ?? must try bigger preplexities as the dataset is big ~500\n",
        "    np.set_printoptions(suppress=True)\n",
        "    Y = tsne.fit_transform(Vec)\n",
        "    x_coords = Y[:, 0]\n",
        "    y_coords = Y[:, 1]\n",
        "    z_coords = Y[:, 2]\n",
        "    return x_coords, y_coords, z_coords\n",
        "\n",
        "# changed to Vec_new\n",
        "x1, y1, z1 = get_coordinates(Vec_2)\n",
        "x2, y2, z2 = get_coordinates(Vec_3)\n",
        "x3, y3, z3 = get_coordinates(Vec_4)\n",
        "x4, y4, z4 = get_coordinates(Vec_5)\n",
        "x5, y5, z5 = get_coordinates(Vec_6)\n",
        "\n",
        "plot1 = go.Scatter3d(x = x1,\n",
        "                     y = y1, \n",
        "                     z = z1,\n",
        "                     mode = 'markers+text',\n",
        "                     text = simil_final_2, \n",
        "                     textposition = 'bottom center',\n",
        "                     hoverinfo = 'text',\n",
        "                     marker = dict(size=5, line=dict(color='rgba(1, 1, 1)', width=1), opacity=0.8))\n",
        "\n",
        "plot2 = go.Scatter3d(x = x2+500,\n",
        "                     y = y2,\n",
        "                     z = z2+500,\n",
        "                     mode = 'markers+text',\n",
        "                     text = simil_final_3,\n",
        "                     textposition = 'bottom center',\n",
        "                     hoverinfo = 'text',\n",
        "                     marker = dict(size=5, line=dict(color='rgb(30, 30, 30)', width=1), opacity=0.9))\n",
        "\n",
        "plot3 = go.Scatter3d(x = x3,\n",
        "                     y = y3-1000,\n",
        "                     z = z3,\n",
        "                     mode = 'markers+text',\n",
        "                     text = simil_final_4,\n",
        "                     textposition = 'bottom center',\n",
        "                     hoverinfo = 'text',\n",
        "                     marker = dict(size=5, line=dict(color='rgb(70, 70, 70)', width=1), opacity=0.9))\n",
        "\n",
        "plot4 = go.Scatter3d(x = x4+1000,\n",
        "                     y = y4,\n",
        "                     z = z4+1000,\n",
        "                     mode = 'markers+text',\n",
        "                     text = simil_final_5,\n",
        "                     textposition = 'bottom center',\n",
        "                     hoverinfo = 'text',\n",
        "                     marker = dict(size=5, line=dict(color='rgb(100, 100, 100)', width=1), opacity=0.9))\n",
        "\n",
        "plot5 = go.Scatter3d(x = x5,\n",
        "                     y = y5+1000, \n",
        "                     z = z5,\n",
        "                     mode = 'markers+text',\n",
        "                     text = simil_final_6, \n",
        "                     textposition = 'bottom center',\n",
        "                     hoverinfo = 'text',\n",
        "                     marker = dict(size=5, line=dict(color='rgba(150, 150, 150)', width=1), opacity=0.8))\n",
        "\n",
        "data = [plot1, plot2, plot3, plot4, plot5]\n",
        "layout = go.Layout(title='Clusters of various similar words in 3D space')\n",
        "fig = go.Figure(data=data, layout=layout)\n",
        "plt2.iplot(fig)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S5FX_xpYrZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clustring on conv (cluster the words using KMeans--> clustering algorithm look at differences between vectors/centers)\n",
        "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
        "    # Initalize a k-means object and use it to extract centroids\n",
        "    kmeans_clustering = KMeans(n_clusters=num_clusters, init='k-means++', \n",
        "                               n_init=100, max_iter=500, tol=0.0001, verbose=0)\n",
        "    idx = kmeans_clustering.fit_predict(word_vectors)\n",
        "    return kmeans_clustering.cluster_centers_, idx\n",
        "\n",
        "num_clusters = 20\n",
        "centers, clusters = clustering_on_wordvecs(Vec, num_clusters)\n",
        "centroid_map = dict(zip(words_filtered_conv_new, clusters))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OPOU8h9lfay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_words(index2word, k, centers, wordvecs):\n",
        "    tree = KDTree(wordvecs)\n",
        "    # Closest points for each Cluster center is used to query the closest 20 points to it.\n",
        "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers]\n",
        "    closest_words_idxs = [x[1] for x in closest_points]\n",
        "    # Word Index is queried for each position in the above array, and added to a Dictionary.\n",
        "    closest_words = {}\n",
        "    for i in range(0, len(closest_words_idxs)):\n",
        "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
        "    # A DataFrame is generated from the dictionary.\n",
        "    df1 = pd.DataFrame(closest_words)\n",
        "    return df1\n",
        "\n",
        "top_words = get_top_words(words_filtered_conv_new, int(len(clusters)/500), centers, Vec) # 100< smallll\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Wc09hLrjdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generating 20 clusters using kmeans clustering which are representing \n",
        "# similar words in each cluster\n",
        "def display_cloud(cluster_num, cmap):\n",
        "    wc = WordCloud(background_color=\"black\", max_words=350, max_font_size=60, colormap=cmap);\n",
        "    wordcloud = wc.generate(' '.join([word for word in top_words['Cluster #' + str(cluster_num)]]))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "    plt.savefig('cluster_' + str(cluster_num), bbox_inches='tight')\n",
        "\n",
        "# colors setting\n",
        "cmaps = cycle(['flag', 'prism', 'ocean', 'gist_earth', 'terrain', 'gist_stern',\n",
        "               'gnuplot', 'gnuplot2', 'CMRmap', 'cubehelix', 'brg', 'hsv',\n",
        "               'gist_rainbow', 'rainbow', 'jet', 'nipy_spectral', 'gist_ncar'])\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    col = next(cmaps)\n",
        "    display_cloud(i, col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s796dDVQW4xK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new: Document (title) to Vec\n",
        "# Averaging Word Embeddings method\n",
        "def document_vector(word2vec_model, doc):\n",
        "    # remove out-of-vocabulary words\n",
        "    doc = [word for word in doc if word in model.vocab]\n",
        "    return np.mean(model[doc], axis=0)\n",
        "\n",
        "# Our earlier preprocessing was done when we were dealing only with word vectors, here, we need each document to remain a document \n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# Function that will help us drop documents that have no word vectors in word2vec\n",
        "def has_vector_representation(word2vec_model, doc):\n",
        "    \"\"\"check if at least one word of the document is in the w2v dictionary\"\"\"\n",
        "    return not all(word not in word2vec_model.vocab for word in doc)\n",
        "\n",
        "# Filter out documents\n",
        "def filter_docs(corpus, texts, condition_on_doc):\n",
        "    \"\"\"\n",
        "    Filter corpus and texts given the function condition_on_doc which takes a doc. The document doc is kept if condition_on_doc(doc) is true.\n",
        "    \"\"\"\n",
        "    number_of_docs = len(corpus)\n",
        "    if texts is not None:\n",
        "       texts = [text for (text, doc) in zip(texts, corpus) if condition_on_doc(doc)]\n",
        "\n",
        "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
        "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
        "    return (corpus, texts)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8N-LE8UeUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocess the corpus\n",
        "# \"titles_list\" is already generated\n",
        "corpus = [preprocess(title) for title in titles_list]\n",
        "\n",
        "# Remove docs that don't include any words in W2V's vocab\n",
        "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n",
        "\n",
        "# Filter out any empty docs\n",
        "corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))\n",
        "x = []\n",
        "for doc in corpus: # append the vector for each document\n",
        "    x.append(document_vector(model, doc))\n",
        "    \n",
        "X = np.array(x) # list to array\n",
        "print(len(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWvGLAnBZ1BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# t-SNE, Round 2: Document Vectors\n",
        "# Initialize t-SNE\n",
        "tsne = TSNE(n_components = 2, init = 'pca', random_state = 0, perplexity = 100)   #best set of parameters\n",
        "len_arr = 6000 #represent ~4-6 months of forum\n",
        "\n",
        "for i in range(12):\n",
        "    plt.close()\n",
        "    h = len_arr*i\n",
        "    tsne_df = tsne.fit_transform(X[h:h+len_arr])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (20, 15))\n",
        "    sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.3)\n",
        "    texts = []\n",
        "    titles_to_plot = list(np.arange(0, len_arr, 400)) \n",
        "    # Append words to list\n",
        "    for title in titles_to_plot:\n",
        "        texts.append(plt.text(tsne_df[title, 0], tsne_df[title, 1], titles_list[title+h], fontsize = 13)) \n",
        "    \n",
        "    # Plot text using adjust_text\n",
        "    adjust_text(texts, force_points = 0.5, force_text = 0.5, \n",
        "                expand_points = (2,1), expand_text = (1,2),\n",
        "                arrowprops = dict(arrowstyle = \"-\", color = 'red', lw = 0.6))\n",
        "    \n",
        "    plt.savefig(\"img{:02}.png\".format(i), dpi=300)\n",
        "    plt.show()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6cYvMvRMjXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Cluster the titles using KMeans--> clustering algorithm look at differences between vectors/centers)\n",
        "# def clustering_on_wordvecs(word_vectors, num_clusters):\n",
        "#     # Initalize a k-means object and use it to extract centroids\n",
        "#     kmeans_clustering = KMeans(n_clusters=num_clusters, init='k-means++', \n",
        "#                                n_init=100, max_iter=500, tol=0.0001, verbose=0)\n",
        "#     idx = kmeans_clustering.fit_predict(word_vectors)\n",
        "    \n",
        "#     return kmeans_clustering.cluster_centers_, idx\n",
        "\n",
        "# num_clusters = 5\n",
        "# # X represents the new vector for docs (titles)\n",
        "# # titles_list represents each title in a list\n",
        "# centers, clusters = clustering_on_wordvecs(X, num_clusters) #on half of the X\n",
        "# centroid_map = dict(zip(titles_list, clusters))\n",
        "# print('Done!', num_clusters) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo_staZZyio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_top_titles(index2word, k, centers, wordvecs):\n",
        "#     tree = KDTree(wordvecs)\n",
        "#     # Closest points for each Cluster center is used to query the closest 20 points to it.\n",
        "#     closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers]\n",
        "#     closest_words_idxs = [x[1] for x in closest_points]\n",
        "#     # Word Index is queried for each position in the above array, and added to a Dictionary.\n",
        "#     closest_words = {}\n",
        "#     for i in range(0, len(closest_words_idxs)):\n",
        "#         closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
        "#     # A DataFrame is generated from the dictionary.\n",
        "#     df1 = pd.DataFrame(closest_words)\n",
        "#     return df1\n",
        "\n",
        "# top_titles = get_top_titles(titles_list, len(clusters), centers, X) \n",
        "\n",
        "# # Separate each top_titles into a specific category\n",
        "# online_list = top_titles[0]\n",
        "# len(online_list)\n",
        "\n",
        "# idea_list = top_titles[1]\n",
        "# len(idea_list)\n",
        "\n",
        "# opinion_list = top_titles[2]\n",
        "# len(opinion_list)\n",
        "\n",
        "# sale_list = top_titles[3]\n",
        "# len(sale_list)\n",
        "\n",
        "# marketing_list = top_titles[4]\n",
        "# len(marketing_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HL-29l4aLTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = title_data[title_data['Title'].str.contains(\"e-commerce\") | title_data['Title'].str.contains(\"E-commerce\") | title_data['Title'].str.contains(\"digital\") | title_data['Title'].str.contains(\"social media\") | title_data['Title'].str.contains(\"platform\") |title_data['Title'].str.contains(\"blog\") | title_data['Title'].str.contains(\"website\") | title_data['Title'].str.contains('amazon')  | title_data['Title'].str.contains('Amazon') | title_data['Title'].str.contains('online') | title_data['Title'].str.contains('site')]\n",
        "idea_titles = df1['Title']\n",
        "online_list = [title for title in idea_titles]\n",
        "len(online_list)\n",
        "\n",
        "df1 = title_data[title_data['Title'].str.contains(\"idea\") | title_data['Title'].str.contains(\"plan\")]\n",
        "idea_titles = df1['Title']\n",
        "idea_list = [title for title in idea_titles]\n",
        "len(idea_list)\n",
        "\n",
        "df1 = title_data[title_data['Title'].str.contains(\"feedbacks\") | title_data['Title'].str.contains(\"feedback\") | title_data['Title'].str.contains(\"opinion\") | title_data['Title'].str.contains(\"advice\") | title_data['Title'].str.contains('suggestion') | title_data['Title'].str.contains(\"recommendation\") | title_data['Title'].str.contains(\"opinions\") | title_data['Title'].str.contains(\"advices\") | title_data['Title'].str.contains('suggestions') | title_data['Title'].str.contains(\"recommendations\") ]\n",
        "idea_titles = df1['Title']\n",
        "opinion_list = [title for title in idea_titles]\n",
        "len(opinion_list)\n",
        "\n",
        "df1 = title_data[title_data['Title'].str.contains(\"sale\") | title_data['Title'].str.contains(\"sales\")]\n",
        "idea_titles = df1['Title']\n",
        "sale_list = [title for title in idea_titles]\n",
        "len(sale_list)\n",
        "\n",
        "df1 = title_data[title_data['Title'].str.contains(\"marketing\")]\n",
        "idea_titles = df1['Title']\n",
        "marketing_list = [title for title in idea_titles]\n",
        "len(marketing_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIWfvv_FOmSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating dictionary with mentioned categories\n",
        "foo_0 = {'E-commerce': online_list,\n",
        "         'Business Idea': idea_list,\n",
        "         'Opinions/Feedbacks': opinion_list,\n",
        "         'Sales': sale_list,\n",
        "         'Marketing': marketing_list}\n",
        "\n",
        "foo_1 = pd.DataFrame.from_dict(foo_0, orient='index')\n",
        "foo_1 = foo_1.transpose()\n",
        "foo_1.to_csv(\"list_titles.csv\", index=False)\n",
        "\n",
        "len_rest = len(X) - int((len(online_list)+len(idea_list)+len(opinion_list)+len(sale_list)+len(marketing_list))*1.0)\n",
        "labels = ['E-commerce', 'Business Idea', 'Opinions/Feedbacks', 'Sales', 'Marketing', 'The rest']\n",
        "values = [int(len(online_list)*1.0), int(len(idea_list)*1.0), int(len(opinion_list)*1.0), int(len(sale_list)*1.0), int(len(marketing_list)*1.0), len_rest]\n",
        "# Pie chart with Plotly\n",
        "fig = go.Figure(data=[go.Pie(labels=labels, values=values, pull=[0, 0.1, 0, 0, 0, 0])])\n",
        "fig.update_layout(title=\"The percentage of 'Business Idea' titles versus the other hot topics: \")\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c1HmbRBbAZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding “BizIdea” which is binary with 1 being in the “Business Ideas” cluster and 0 being not\n",
        "df['BizIdea'] = np.nan\n",
        "list_titles = list(df[\"Title\"])\n",
        "\n",
        "for i in range(len(list_titles)):\n",
        "    if list_titles[i] in idea_list:\n",
        "       df['BizIdea'].values[i] = 1\n",
        "    else:\n",
        "       df['BizIdea'].values[i] = 0\n",
        "\n",
        "out5 = df.to_csv('output5.csv', index = None, header=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEP8E3Vdotuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# related them together"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRusKjVvmNoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new = pd.read_csv('/content/drive/My Drive/WS_nlp/MyData.csv', engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "for i in df_new.columns.values:\n",
        "  T[i] = df_new.columns.values[i]\n",
        "# print(T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3YKm1Bkoqxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do not run (a comment chunk)\n",
        "# topic 1\n",
        "'idea market book step course agency youtube area lead capital firm channel hack run guidance scratch reality minute validation content passion flippa size train growth date creation worker brainstorm inspire'\n",
        "# topic 2\n",
        "'experience time pay thing tool subscription box resource kickstarter opinion guide skill investment value franchise break production check spend art record sticker fba factory builder magazine hold advertisement beer bag'\n",
        "# topic 3\n",
        "'product interest amazon deal life purchase drop datum space world office mail photo struggle seller family member scale concept nothing location right expand expense raise dropshipper morning operate produce ppc'\n",
        "# topic 4\n",
        "'start online business design price story development offer manufacture restaurant web delivery technology moment mastermind season holiday broker position export woocommerce ticket accountant impact proposal potential slack compare foot prep'\n",
        "# topic 5\n",
        "'sale feedback customer partner project today tax strategy form change license detail term register copy stay motivation distributor asium government stand rep agent premium automate exist apartment prelaunch cut motivate'\n",
        "# topic 6\n",
        "'service list email account instagram review card case study credit example issue follower twitter failure relate setup photography info vendor party paper database limit couple mobile forum enter text connection'\n",
        "# topic 7\n",
        "'website software base cost reddit investor custom host target feedback feel xpost event inventory rstartup file prototype shirt quality room management bar insight commerce iphone criticism progress bring document secure'\n",
        "# topic 8\n",
        "'app ecommerce store blog china shopify venture equity article online point result march gift write noob blogger dream mistake curiou movie split innovation giveaway usa squarespace seek wonder shoot pain'\n",
        "# topic 9\n",
        "'person video industry hire llc food content car cofounder survey show outsource structure everyone partnership trade support join flip piece message percentage july reseller power task con acquisition click response'\n",
        "# topic 10\n",
        "'startup facebook niche place kind patent country solution talk competition application manager read image bill invoice man estate visitor generation begin decision supplement return career writer flyer expert require boost'\n",
        "# topic 11\n",
        "'page land game marketplace network import contact competitor track freelance tech journey insurance end taxe mentor board difference series challenge focu somebody copyright andor answer summer energy display submit coach'\n",
        "# topic 12\n",
        "'advice research college group student conversion user validate audience practice machine mine theme fulfillment gain australium reason town hmy invention send volume join round contest night scam path cold provide'\n",
        "# topic 13\n",
        "'name brand launch plan seo friend interview promote domain link bank house growth loan marketer internet retailer ceo trend europe grant switch choose auto liability accelerator presence description trainer background'\n",
        "# topic 14\n",
        "'site create recommendation critique employee traffic package feature method finance freelancer discussion niche input computer repair field sort oversea woman hair pokemon gold programmer experiment dog session burn poster avoid'\n",
        "# topic 15\n",
        "'tip learn web developer side lesson buy manufacturer request alibaba increase dollar accomplishment project vium dev university buyer productivity june pursue trial distribution hear bulk focus parent deck security equipment'\n",
        "# topic 16\n",
        "'sell suggestion shop income source print search tshirt model regard move state ebook online ebay demand schedule watch corporation rent keyword amount custom style enterprise leverage notice stop merchandise fire'\n",
        "# topic 17\n",
        "'money medium job management founder community hour quit recommend budget margin advantage stage future contractor fashion discount promotion portfolio weekend commission chance brick simple decide tomorrow mortar kid subscriber transfer'\n",
        "# topic 18\n",
        "'affiliate build payment profit program domain anybody type worth drive supplier hunt adword turn fail amazon biz law coffee traffic monetization pick technique direction truck stripe visit claim artist legality'\n",
        "# topic 19\n",
        "'ship platform order campaign problem home system approach pitch manage consult goal stuff figure property update apparel organization sport hand item thread letter youve printer poll conference ability face fix'\n",
        "# topic 20\n",
        "'post owner advertise google opportunity revenue grow word trademark facebook fee blog subreddit adsense betum newsletter reach view merchant sample news rank etsy resell content exchange stick trump topic'\n",
        "# topic 21\n",
        "'share client option team podcast rate phone information contract stock number canada risk sign comment process relationship lease stream door agreement picture attempt crm estimate calculate foundation cannabis miss'\n",
        "# topic 22\n",
        "'fund clothe school item line charge monetize label generate travel engineer meet knowledge influencer assistant trouble benefit paypal situation quote californium health level center consumer matter rule adult webinar'\n",
        "# topic 23\n",
        "'test logo designer city b2b consultant sub cash code template handle music care release report download beginner mind mvp fun lawyer teach hate edit tea waste supply material construction convert'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP0WPUdVTCeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Topic modeling\n",
        "# X has been created in the previous project (document vector)\n",
        "topic = X\n",
        "topic_vec = topic[0:int(0.8*len(X))]\n",
        "train_vecs = []\n",
        "for i in range(len(topic_vec)):\n",
        "    # frequency of 0.8 chosen\n",
        "    topic_vec = [topic_vec[i] for i in range(23)]\n",
        "    train_vecs.append(topic_vec)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YsWT6EfTChB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training a Supervised Classifier\n",
        "X = np.array(train_vecs)\n",
        "y = np.array(df_new.target)\n",
        "\n",
        "kf = KFold(5, shuffle=True, random_state=42)\n",
        "cv_lr_f1, cv_lrsgd_f1, cv_svcsgd_f1,  = [], [], []\n",
        "\n",
        "for train_ind, val_ind in kf.split(X, y):\n",
        "    # Assign CV IDX\n",
        "    X_train, y_train = X[train_ind], y[train_ind]\n",
        "    X_val, y_val = X[val_ind], y[val_ind]\n",
        "    \n",
        "    # Scale Data\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scale = scaler.fit_transform(X_train)\n",
        "    X_val_scale = scaler.transform(X_val)\n",
        "\n",
        "    # Logisitic Regression\n",
        "    lr = LogisticRegression(\n",
        "        class_weight= 'balanced',\n",
        "        solver='newton-cg',\n",
        "        fit_intercept=True\n",
        "    ).fit(X_train_scale, y_train)\n",
        "\n",
        "    y_pred = lr.predict(X_val_scale)\n",
        "    cv_lr_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
        "    \n",
        "    # Logistic Regression SGD\n",
        "    sgd = linear_model.SGDClassifier(\n",
        "        max_iter=1000,\n",
        "        tol=1e-3,\n",
        "        loss='log',\n",
        "        class_weight='balanced'\n",
        "    ).fit(X_train_scale, y_train)\n",
        "    \n",
        "    y_pred = sgd.predict(X_val_scale)\n",
        "    cv_lrsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n",
        "    \n",
        "    # SGD Modified Huber\n",
        "    sgd_huber = linear_model.SGDClassifier(\n",
        "        max_iter=1000,\n",
        "        tol=1e-3,\n",
        "        alpha=23,\n",
        "        loss='modified_huber',\n",
        "        class_weight='balanced'\n",
        "    ).fit(X_train_scale, y_train)\n",
        "    \n",
        "    y_pred = sgd_huber.predict(X_val_scale)\n",
        "    cv_svcsgd_f1.append(f1_score(y_val, y_pred, average='binary'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-UPYzY8TClj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Applying the Model on Unseen Data\n",
        "\n",
        "def get_bigram(df):\n",
        "    df['text'] = strip_newline(df.text)\n",
        "    words = list(sent_to_words(df.text))\n",
        "    words = remove_stopwords(words)\n",
        "    bigram = bigrams(words)\n",
        "    bigram = [bigram[review] for review in words]\n",
        "    return bigram\n",
        "  \n",
        "bigram_test = get_bigram(df_new)\n",
        "\n",
        "test_corpus = [train_id2word.doc2bow(text) for text in bigram_test]\n",
        "\n",
        "topic_vec = topic[int(0.8*len(X)):]\n",
        "test_vecs = []\n",
        "\n",
        "for i in range(len(topic_vec)):\n",
        "    topic_vec = [topic_vec[i] for i in range(23)]\n",
        "    topic_vec.extend([df_new.iloc[i].real_counts]) \n",
        "    topic_vec.extend([len(df_new.iloc[i].text)]) \n",
        "    test_vecs.append(topic_vec)\n",
        "\n",
        "\n",
        "# create a dataset with non NAN values for replies\n",
        "replier_data = df[df['Replier'].notnull()]\n",
        "replier_data['Topic Number'] = np.nan\n",
        "for topic in T:\n",
        "  temp = [0]*len(replier_conv_list)\n",
        "  n += 1\n",
        "  for i in range(len(replier_conv_list)):\n",
        "      temp[i] = [word for word in topic if word in replier_words_conv[i]]\n",
        "      replier_data['Topic Number'].values[i] = n\n",
        "\n",
        "out = replier_data.to_csv('topic_reply.csv', index = None, header = True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ_HMU4ATCpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['Topic Loading'] = np.nan\n",
        "data['Topic Loading'] = data['Topic Loading'].astype(object)\n",
        "\n",
        "# Create a 23 by 2 array\n",
        "a = np.ndarray(shape=(23,2))\n",
        "a[:,0] = range(1,24)\n",
        "\n",
        "# Vectors of replies: \"topic\" (3 chunks above)\n",
        "# Run this loop on Cedar or other HPCs (with 60G RAM-CPU on Cedar took around 23 hours to be completed)\n",
        "for i in range(len(data)):\n",
        "    if data['Topic Number'].notnull().values[i]:\n",
        "        aa = a\n",
        "        temp_ = data['Topic Number'].values[i]\n",
        "        for vector in topic:\n",
        "          for j in vector:\n",
        "            norm[j] = float(vector[j])/sum(vector) #23 classes\n",
        "            aa[j,1] = norm[j]\n",
        "        if int(max(aa[:,1])) != int(aa[temp_-1,1]):\n",
        "            print(\"Oops!\")\n",
        "            break\n",
        "        b = np.ndarray.tolist(aa)\n",
        "        # treated as an object [b]\n",
        "        data.loc[i, 'Topic Loading'] = [b]\n",
        "        # check if the code is running\n",
        "        if i%10000 == 0:\n",
        "          print(i)\n",
        "\n",
        "out_2 = replier_data.to_csv('Topic_Loading.csv', index=None, header=True)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJCsp4meTCkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}